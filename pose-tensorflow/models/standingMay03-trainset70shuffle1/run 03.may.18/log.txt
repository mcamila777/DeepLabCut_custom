2018-05-03 14:28:46 Config:
{'all_joints': [[0]],
 'all_joints_names': ['knee'],
 'batch_size': 1,
 'crop': False,
 'crop_pad': 0,
 'dataset': '../../UnaugmentedDataSet_standingMay03/standing_camila70shuffle1.mat',
 'dataset_type': 'default',
 'display_iters': 1000,
 'fg_fraction': 0.25,
 'global_scale': 0.8,
 'init_weights': '../../pretrained/resnet_v1_50.ckpt',
 'intermediate_supervision': False,
 'intermediate_supervision_layer': 12,
 'location_refinement': True,
 'locref_huber_loss': True,
 'locref_loss_weight': 0.05,
 'locref_stdev': 7.2801,
 'log_dir': 'log',
 'max_input_size': 1000,
 'mean_pixel': [123.68, 116.779, 103.939],
 'mirror': False,
 'multi_step': [[0.005, 10000],
                [0.02, 430000],
                [0.002, 730000],
                [0.001, 1030000]],
 'net_type': 'resnet_50',
 'num_joints': 1,
 'optimizer': 'sgd',
 'pos_dist_thresh': 17,
 'regularize': False,
 'save_iters': 50000,
 'scale_jitter_lo': 0.5,
 'scale_jitter_up': 1.5,
 'scoremap_dir': 'test',
 'shuffle': True,
 'snapshot_prefix': './snapshot',
 'stride': 8.0,
 'use_gt_segm': False,
 'video': False,
 'video_batch': False,
 'weigh_negatives': False,
 'weigh_only_present_joints': False,
 'weigh_part_predictions': False,
 'weight_decay': 0.0001}
2018-05-03 14:28:46 From /is/ps2/calvarez2/DeepLabCut/pose-tensorflow/nnet/pose_net.py:52: calling resnet_arg_scope (from tensorflow.contrib.slim.python.slim.nets.resnet_utils) with is_training is deprecated and will be removed after 2017-08-01.
Instructions for updating:
Pass is_training directly to the network instead of the arg_scope.
2018-05-03 14:28:47 logits.dtype=<dtype: 'float32'>.
2018-05-03 14:28:47 multi_class_labels.dtype=<dtype: 'float32'>.
2018-05-03 14:28:47 losses.dtype=<dtype: 'float32'>.
2018-05-03 14:28:57 Restoring parameters from ../../pretrained/resnet_v1_50.ckpt
2018-05-03 14:29:01 iteration: 0 loss: 0.0009 lr: 0.005
2018-05-03 14:31:24 iteration: 1000 loss: 0.0255 lr: 0.005
2018-05-03 14:33:50 iteration: 2000 loss: 0.0143 lr: 0.005
2018-05-03 14:36:13 iteration: 3000 loss: 0.0107 lr: 0.005
2018-05-03 14:38:39 iteration: 4000 loss: 0.0088 lr: 0.005
2018-05-03 14:41:04 iteration: 5000 loss: 0.0071 lr: 0.005
2018-05-03 14:43:26 iteration: 6000 loss: 0.0060 lr: 0.005
2018-05-03 14:45:54 iteration: 7000 loss: 0.0055 lr: 0.005
2018-05-03 14:48:22 iteration: 8000 loss: 0.0050 lr: 0.005
2018-05-03 14:50:47 iteration: 9000 loss: 0.0045 lr: 0.005
2018-05-03 14:53:20 iteration: 10000 loss: 0.0042 lr: 0.005
2018-05-03 14:55:44 iteration: 11000 loss: 0.0125 lr: 0.02
2018-05-03 14:58:12 iteration: 12000 loss: 0.0089 lr: 0.02
2018-05-03 15:00:39 iteration: 13000 loss: 0.0068 lr: 0.02
2018-05-03 15:03:08 iteration: 14000 loss: 0.0059 lr: 0.02
2018-05-03 15:05:34 iteration: 15000 loss: 0.0051 lr: 0.02
2018-05-03 15:08:01 iteration: 16000 loss: 0.0048 lr: 0.02
2018-05-03 15:10:24 iteration: 17000 loss: 0.0044 lr: 0.02
2018-05-03 15:12:53 iteration: 18000 loss: 0.0040 lr: 0.02
2018-05-03 15:15:21 iteration: 19000 loss: 0.0038 lr: 0.02
2018-05-03 15:17:49 iteration: 20000 loss: 0.0039 lr: 0.02
2018-05-03 15:20:19 iteration: 21000 loss: 0.0038 lr: 0.02
2018-05-03 15:22:47 iteration: 22000 loss: 0.0035 lr: 0.02
2018-05-03 15:25:14 iteration: 23000 loss: 0.0033 lr: 0.02
2018-05-03 15:27:42 iteration: 24000 loss: 0.0031 lr: 0.02
2018-05-03 15:30:14 iteration: 25000 loss: 0.0031 lr: 0.02
2018-05-03 15:32:42 iteration: 26000 loss: 0.0029 lr: 0.02
2018-05-03 15:35:06 iteration: 27000 loss: 0.0029 lr: 0.02
2018-05-03 15:37:35 iteration: 28000 loss: 0.0029 lr: 0.02
2018-05-03 15:40:00 iteration: 29000 loss: 0.0029 lr: 0.02
2018-05-03 15:42:28 iteration: 30000 loss: 0.0028 lr: 0.02
2018-05-03 15:44:59 iteration: 31000 loss: 0.0027 lr: 0.02
2018-05-03 15:47:29 iteration: 32000 loss: 0.0027 lr: 0.02
2018-05-03 15:49:55 iteration: 33000 loss: 0.0027 lr: 0.02
2018-05-03 15:52:24 iteration: 34000 loss: 0.0025 lr: 0.02
2018-05-03 15:54:50 iteration: 35000 loss: 0.0026 lr: 0.02
2018-05-03 15:57:16 iteration: 36000 loss: 0.0024 lr: 0.02
2018-05-03 15:59:44 iteration: 37000 loss: 0.0025 lr: 0.02
2018-05-03 16:02:11 iteration: 38000 loss: 0.0024 lr: 0.02
2018-05-03 16:04:35 iteration: 39000 loss: 0.0023 lr: 0.02
2018-05-03 16:07:03 iteration: 40000 loss: 0.0024 lr: 0.02
2018-05-03 16:09:31 iteration: 41000 loss: 0.0025 lr: 0.02
2018-05-03 16:12:00 iteration: 42000 loss: 0.0025 lr: 0.02
2018-05-03 16:14:24 iteration: 43000 loss: 0.0023 lr: 0.02
2018-05-03 16:16:51 iteration: 44000 loss: 0.0023 lr: 0.02
2018-05-03 16:19:18 iteration: 45000 loss: 0.0023 lr: 0.02
2018-05-03 16:21:47 iteration: 46000 loss: 0.0023 lr: 0.02
2018-05-03 16:24:12 iteration: 47000 loss: 0.0022 lr: 0.02
2018-05-03 16:26:38 iteration: 48000 loss: 0.0022 lr: 0.02
2018-05-03 16:29:05 iteration: 49000 loss: 0.0022 lr: 0.02
2018-05-03 16:31:33 iteration: 50000 loss: 0.0022 lr: 0.02
2018-05-03 16:34:03 iteration: 51000 loss: 0.0021 lr: 0.02
2018-05-03 16:36:31 iteration: 52000 loss: 0.0022 lr: 0.02
2018-05-03 16:38:54 iteration: 53000 loss: 0.0020 lr: 0.02
2018-05-03 16:41:19 iteration: 54000 loss: 0.0021 lr: 0.02
2018-05-03 16:43:44 iteration: 55000 loss: 0.0021 lr: 0.02
2018-05-03 16:46:12 iteration: 56000 loss: 0.0020 lr: 0.02
2018-05-03 16:48:37 iteration: 57000 loss: 0.0021 lr: 0.02
2018-05-03 16:51:03 iteration: 58000 loss: 0.0020 lr: 0.02
2018-05-03 16:53:31 iteration: 59000 loss: 0.0020 lr: 0.02
