2018-04-19 17:21:48 Config:
{'all_joints': [[0], [1], [2], [3]],
 'all_joints_names': ['hand', 'Finger1', 'Finger2', 'Joystick'],
 'batch_size': 1,
 'crop': False,
 'crop_pad': 0,
 'dataset': '../../UnaugmentedDataSet_reachingJan30/reaching_Mackenzie95shuffle1.mat',
 'dataset_type': 'default',
 'display_iters': 5000,
 'fg_fraction': 0.25,
 'global_scale': 0.8,
 'init_weights': '../../pretrained/resnet_v1_50.ckpt',
 'intermediate_supervision': False,
 'intermediate_supervision_layer': 12,
 'location_refinement': True,
 'locref_huber_loss': True,
 'locref_loss_weight': 0.05,
 'locref_stdev': 7.2801,
 'log_dir': 'log',
 'max_input_size': 1000,
 'mean_pixel': [123.68, 116.779, 103.939],
 'mirror': False,
 'multi_step': [[0.005, 10000],
                [0.02, 430000],
                [0.002, 730000],
                [0.001, 1030000]],
 'net_type': 'resnet_50',
 'num_joints': 4,
 'optimizer': 'sgd',
 'pos_dist_thresh': 17,
 'regularize': False,
 'save_iters': 50000,
 'scale_jitter_lo': 0.5,
 'scale_jitter_up': 1.5,
 'scoremap_dir': 'test',
 'shuffle': True,
 'snapshot_prefix': './snapshot',
 'stride': 8.0,
 'use_gt_segm': False,
 'video': False,
 'video_batch': False,
 'weigh_negatives': False,
 'weigh_only_present_joints': False,
 'weigh_part_predictions': False,
 'weight_decay': 0.0001}
2018-04-19 17:21:48 From /is/ps2/calvarez2/DeepLabCut/pose-tensorflow/nnet/pose_net.py:52: calling resnet_arg_scope (from tensorflow.contrib.slim.python.slim.nets.resnet_utils) with is_training is deprecated and will be removed after 2017-08-01.
Instructions for updating:
Pass is_training directly to the network instead of the arg_scope.
2018-04-19 17:21:49 logits.dtype=<dtype: 'float32'>.
2018-04-19 17:21:49 multi_class_labels.dtype=<dtype: 'float32'>.
2018-04-19 17:21:49 losses.dtype=<dtype: 'float32'>.
2018-04-19 17:21:55 Restoring parameters from ../../pretrained/resnet_v1_50.ckpt
2018-04-19 17:22:01 iteration: 0 loss: 0.0002 lr: 0.005
2018-04-19 17:29:56 iteration: 5000 loss: 0.0086 lr: 0.005
2018-04-19 17:38:02 iteration: 10000 loss: 0.0036 lr: 0.005
2018-04-19 17:45:54 iteration: 15000 loss: 0.0041 lr: 0.02
2018-04-19 17:53:37 iteration: 20000 loss: 0.0026 lr: 0.02
2018-04-19 18:01:15 iteration: 25000 loss: 0.0022 lr: 0.02
2018-04-19 18:08:55 iteration: 30000 loss: 0.0019 lr: 0.02
2018-04-19 18:16:38 iteration: 35000 loss: 0.0017 lr: 0.02
2018-04-19 18:24:21 iteration: 40000 loss: 0.0016 lr: 0.02
2018-04-19 18:32:00 iteration: 45000 loss: 0.0015 lr: 0.02
2018-04-19 18:39:41 iteration: 50000 loss: 0.0014 lr: 0.02
2018-04-19 18:47:25 iteration: 55000 loss: 0.0013 lr: 0.02
2018-04-19 18:55:04 iteration: 60000 loss: 0.0012 lr: 0.02
2018-04-19 19:02:45 iteration: 65000 loss: 0.0012 lr: 0.02
2018-04-19 19:10:30 iteration: 70000 loss: 0.0011 lr: 0.02
2018-04-19 19:18:09 iteration: 75000 loss: 0.0011 lr: 0.02
2018-04-19 19:25:52 iteration: 80000 loss: 0.0010 lr: 0.02
2018-04-19 19:33:32 iteration: 85000 loss: 0.0010 lr: 0.02
2018-04-19 19:41:08 iteration: 90000 loss: 0.0010 lr: 0.02
2018-04-19 19:48:42 iteration: 95000 loss: 0.0009 lr: 0.02
2018-04-19 19:56:21 iteration: 100000 loss: 0.0009 lr: 0.02
2018-04-19 20:04:00 iteration: 105000 loss: 0.0009 lr: 0.02
2018-04-19 20:11:38 iteration: 110000 loss: 0.0008 lr: 0.02
2018-04-19 20:19:20 iteration: 115000 loss: 0.0008 lr: 0.02
2018-04-19 20:26:56 iteration: 120000 loss: 0.0008 lr: 0.02
2018-04-19 20:34:39 iteration: 125000 loss: 0.0008 lr: 0.02
2018-04-19 20:42:17 iteration: 130000 loss: 0.0008 lr: 0.02
2018-04-19 20:49:55 iteration: 135000 loss: 0.0008 lr: 0.02
2018-04-19 20:57:34 iteration: 140000 loss: 0.0008 lr: 0.02
2018-04-19 21:05:16 iteration: 145000 loss: 0.0007 lr: 0.02
2018-04-19 21:12:58 iteration: 150000 loss: 0.0007 lr: 0.02
2018-04-19 21:20:32 iteration: 155000 loss: 0.0007 lr: 0.02
2018-04-19 21:28:14 iteration: 160000 loss: 0.0007 lr: 0.02
2018-04-19 21:35:50 iteration: 165000 loss: 0.0007 lr: 0.02
2018-04-19 21:43:30 iteration: 170000 loss: 0.0007 lr: 0.02
2018-04-19 21:51:12 iteration: 175000 loss: 0.0007 lr: 0.02
2018-04-19 21:58:54 iteration: 180000 loss: 0.0007 lr: 0.02
2018-04-19 22:06:31 iteration: 185000 loss: 0.0007 lr: 0.02
2018-04-19 22:14:06 iteration: 190000 loss: 0.0007 lr: 0.02
2018-04-19 22:21:44 iteration: 195000 loss: 0.0007 lr: 0.02
2018-04-19 22:29:24 iteration: 200000 loss: 0.0006 lr: 0.02
2018-04-19 22:37:05 iteration: 205000 loss: 0.0006 lr: 0.02
2018-04-19 22:44:45 iteration: 210000 loss: 0.0006 lr: 0.02
2018-04-19 22:52:23 iteration: 215000 loss: 0.0006 lr: 0.02
2018-04-19 23:00:02 iteration: 220000 loss: 0.0006 lr: 0.02
2018-04-19 23:07:40 iteration: 225000 loss: 0.0006 lr: 0.02
2018-04-19 23:15:18 iteration: 230000 loss: 0.0006 lr: 0.02
2018-04-19 23:22:59 iteration: 235000 loss: 0.0006 lr: 0.02
2018-04-19 23:30:41 iteration: 240000 loss: 0.0006 lr: 0.02
2018-04-19 23:38:18 iteration: 245000 loss: 0.0006 lr: 0.02
2018-04-19 23:46:00 iteration: 250000 loss: 0.0006 lr: 0.02
2018-04-19 23:53:41 iteration: 255000 loss: 0.0006 lr: 0.02
2018-04-20 00:01:21 iteration: 260000 loss: 0.0006 lr: 0.02
2018-04-20 00:09:03 iteration: 265000 loss: 0.0006 lr: 0.02
2018-04-20 00:16:41 iteration: 270000 loss: 0.0006 lr: 0.02
2018-04-20 00:24:23 iteration: 275000 loss: 0.0006 lr: 0.02
2018-04-20 00:32:02 iteration: 280000 loss: 0.0005 lr: 0.02
2018-04-20 00:39:42 iteration: 285000 loss: 0.0006 lr: 0.02
2018-04-20 00:47:22 iteration: 290000 loss: 0.0005 lr: 0.02
2018-04-20 00:55:01 iteration: 295000 loss: 0.0005 lr: 0.02
2018-04-20 01:02:40 iteration: 300000 loss: 0.0005 lr: 0.02
2018-04-20 01:10:21 iteration: 305000 loss: 0.0005 lr: 0.02
2018-04-20 01:18:03 iteration: 310000 loss: 0.0005 lr: 0.02
2018-04-20 01:25:43 iteration: 315000 loss: 0.0005 lr: 0.02
2018-04-20 01:33:25 iteration: 320000 loss: 0.0005 lr: 0.02
2018-04-20 01:41:07 iteration: 325000 loss: 0.0005 lr: 0.02
2018-04-20 01:48:47 iteration: 330000 loss: 0.0005 lr: 0.02
2018-04-20 01:56:30 iteration: 335000 loss: 0.0005 lr: 0.02
2018-04-20 02:04:13 iteration: 340000 loss: 0.0005 lr: 0.02
2018-04-20 02:11:56 iteration: 345000 loss: 0.0005 lr: 0.02
2018-04-20 02:19:35 iteration: 350000 loss: 0.0005 lr: 0.02
2018-04-20 02:27:20 iteration: 355000 loss: 0.0005 lr: 0.02
2018-04-20 02:35:01 iteration: 360000 loss: 0.0005 lr: 0.02
2018-04-20 02:42:43 iteration: 365000 loss: 0.0005 lr: 0.02
2018-04-20 02:50:21 iteration: 370000 loss: 0.0005 lr: 0.02
2018-04-20 02:58:01 iteration: 375000 loss: 0.0005 lr: 0.02
2018-04-20 03:05:42 iteration: 380000 loss: 0.0005 lr: 0.02
2018-04-20 03:13:19 iteration: 385000 loss: 0.0005 lr: 0.02
2018-04-20 03:20:58 iteration: 390000 loss: 0.0005 lr: 0.02
2018-04-20 03:28:36 iteration: 395000 loss: 0.0005 lr: 0.02
2018-04-20 03:36:17 iteration: 400000 loss: 0.0005 lr: 0.02
2018-04-20 03:43:57 iteration: 405000 loss: 0.0005 lr: 0.02
2018-04-20 03:51:37 iteration: 410000 loss: 0.0005 lr: 0.02
2018-04-20 03:59:17 iteration: 415000 loss: 0.0005 lr: 0.02
2018-04-20 04:06:57 iteration: 420000 loss: 0.0004 lr: 0.02
2018-04-20 04:14:38 iteration: 425000 loss: 0.0004 lr: 0.02
2018-04-20 04:22:20 iteration: 430000 loss: 0.0004 lr: 0.02
2018-04-20 04:30:05 iteration: 435000 loss: 0.0003 lr: 0.002
2018-04-20 04:37:46 iteration: 440000 loss: 0.0003 lr: 0.002
2018-04-20 04:45:24 iteration: 445000 loss: 0.0003 lr: 0.002
2018-04-20 04:53:06 iteration: 450000 loss: 0.0003 lr: 0.002
2018-04-20 05:00:49 iteration: 455000 loss: 0.0003 lr: 0.002
2018-04-20 05:08:28 iteration: 460000 loss: 0.0003 lr: 0.002
2018-04-20 05:16:09 iteration: 465000 loss: 0.0003 lr: 0.002
2018-04-20 05:23:52 iteration: 470000 loss: 0.0003 lr: 0.002
2018-04-20 05:31:26 iteration: 475000 loss: 0.0003 lr: 0.002
2018-04-20 05:39:10 iteration: 480000 loss: 0.0003 lr: 0.002
2018-04-20 05:46:50 iteration: 485000 loss: 0.0003 lr: 0.002
2018-04-20 05:54:30 iteration: 490000 loss: 0.0003 lr: 0.002
2018-04-20 06:02:12 iteration: 495000 loss: 0.0003 lr: 0.002
2018-04-20 06:09:53 iteration: 500000 loss: 0.0003 lr: 0.002
2018-04-20 06:17:30 iteration: 505000 loss: 0.0003 lr: 0.002
2018-04-20 06:25:08 iteration: 510000 loss: 0.0003 lr: 0.002
2018-04-20 06:32:45 iteration: 515000 loss: 0.0003 lr: 0.002
2018-04-20 06:40:25 iteration: 520000 loss: 0.0003 lr: 0.002
2018-04-20 06:48:00 iteration: 525000 loss: 0.0003 lr: 0.002
2018-04-20 06:55:40 iteration: 530000 loss: 0.0003 lr: 0.002
2018-04-20 07:03:19 iteration: 535000 loss: 0.0003 lr: 0.002
2018-04-20 07:10:58 iteration: 540000 loss: 0.0003 lr: 0.002
2018-04-20 07:18:41 iteration: 545000 loss: 0.0003 lr: 0.002
2018-04-20 07:26:19 iteration: 550000 loss: 0.0003 lr: 0.002
2018-04-20 07:33:58 iteration: 555000 loss: 0.0003 lr: 0.002
2018-04-20 07:41:40 iteration: 560000 loss: 0.0003 lr: 0.002
2018-04-20 07:49:19 iteration: 565000 loss: 0.0003 lr: 0.002
2018-04-20 07:56:57 iteration: 570000 loss: 0.0003 lr: 0.002
2018-04-20 08:04:36 iteration: 575000 loss: 0.0003 lr: 0.002
2018-04-20 08:12:07 iteration: 580000 loss: 0.0003 lr: 0.002
2018-04-20 08:19:46 iteration: 585000 loss: 0.0003 lr: 0.002
2018-04-20 08:27:24 iteration: 590000 loss: 0.0003 lr: 0.002
2018-04-20 08:35:07 iteration: 595000 loss: 0.0003 lr: 0.002
2018-04-20 08:42:48 iteration: 600000 loss: 0.0003 lr: 0.002
2018-04-20 08:50:32 iteration: 605000 loss: 0.0003 lr: 0.002
2018-04-20 08:58:11 iteration: 610000 loss: 0.0003 lr: 0.002
2018-04-20 09:05:53 iteration: 615000 loss: 0.0003 lr: 0.002
2018-04-20 09:13:30 iteration: 620000 loss: 0.0003 lr: 0.002
2018-04-20 09:21:06 iteration: 625000 loss: 0.0003 lr: 0.002
2018-04-20 09:28:45 iteration: 630000 loss: 0.0003 lr: 0.002
2018-04-20 09:36:22 iteration: 635000 loss: 0.0003 lr: 0.002
2018-04-20 09:43:58 iteration: 640000 loss: 0.0003 lr: 0.002
2018-04-20 09:51:33 iteration: 645000 loss: 0.0003 lr: 0.002
2018-04-20 09:59:15 iteration: 650000 loss: 0.0003 lr: 0.002
2018-04-20 10:06:58 iteration: 655000 loss: 0.0003 lr: 0.002
2018-04-20 10:14:39 iteration: 660000 loss: 0.0003 lr: 0.002
2018-04-20 10:22:18 iteration: 665000 loss: 0.0003 lr: 0.002
2018-04-20 10:30:00 iteration: 670000 loss: 0.0003 lr: 0.002
2018-04-20 10:37:37 iteration: 675000 loss: 0.0003 lr: 0.002
2018-04-20 10:45:21 iteration: 680000 loss: 0.0003 lr: 0.002
2018-04-20 10:52:57 iteration: 685000 loss: 0.0003 lr: 0.002
2018-04-20 11:00:38 iteration: 690000 loss: 0.0003 lr: 0.002
2018-04-20 11:08:19 iteration: 695000 loss: 0.0003 lr: 0.002
2018-04-20 11:16:01 iteration: 700000 loss: 0.0003 lr: 0.002
2018-04-20 11:23:43 iteration: 705000 loss: 0.0003 lr: 0.002
2018-04-20 11:31:24 iteration: 710000 loss: 0.0003 lr: 0.002
2018-04-20 11:39:04 iteration: 715000 loss: 0.0003 lr: 0.002
2018-04-20 11:46:53 iteration: 720000 loss: 0.0003 lr: 0.002
2018-04-20 11:54:36 iteration: 725000 loss: 0.0003 lr: 0.002
2018-04-20 12:02:16 iteration: 730000 loss: 0.0003 lr: 0.002
2018-04-20 12:09:53 iteration: 735000 loss: 0.0003 lr: 0.001
2018-04-20 12:17:30 iteration: 740000 loss: 0.0003 lr: 0.001
2018-04-20 12:25:11 iteration: 745000 loss: 0.0003 lr: 0.001
2018-04-20 12:32:52 iteration: 750000 loss: 0.0003 lr: 0.001
2018-04-20 12:40:36 iteration: 755000 loss: 0.0003 lr: 0.001
2018-04-20 12:48:18 iteration: 760000 loss: 0.0003 lr: 0.001
2018-04-20 12:56:02 iteration: 765000 loss: 0.0003 lr: 0.001
2018-04-20 13:03:46 iteration: 770000 loss: 0.0002 lr: 0.001
2018-04-20 13:11:34 iteration: 775000 loss: 0.0003 lr: 0.001
2018-04-20 13:19:22 iteration: 780000 loss: 0.0003 lr: 0.001
2018-04-20 13:27:04 iteration: 785000 loss: 0.0003 lr: 0.001
2018-04-20 13:34:42 iteration: 790000 loss: 0.0002 lr: 0.001
2018-04-20 13:42:20 iteration: 795000 loss: 0.0003 lr: 0.001
2018-04-20 13:49:59 iteration: 800000 loss: 0.0003 lr: 0.001
2018-04-20 13:57:47 iteration: 805000 loss: 0.0003 lr: 0.001
2018-04-20 14:05:28 iteration: 810000 loss: 0.0002 lr: 0.001
2018-04-20 14:13:17 iteration: 815000 loss: 0.0002 lr: 0.001
2018-04-20 14:21:08 iteration: 820000 loss: 0.0002 lr: 0.001
